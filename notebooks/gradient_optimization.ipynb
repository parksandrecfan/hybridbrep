{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from hybridbrep import BRepFaceAutoencoder, HPart, WeightedChamferDistance, GeneralConvEncDec\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch_geometric.transforms import SamplePoints\n",
    "from torch_geometric.data import Data as TGData\n",
    "import meshplot as mp\n",
    "from chamferdist import ChamferDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdegravity.net/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoading Model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdegravity.net/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdegravity.net/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(ckpt_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdegravity.net/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m GeneralConvEncDec(\u001b[39m64\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdegravity.net/home/ben/Documents/research/repbrep/hybridbrep/notebooks/gradient_optimization.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(ckpt[\u001b[39m'\u001b[39m\u001b[39mstate_dict\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:607\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    606\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 607\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:882\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    881\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m--> 882\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    884\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:857\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m data_type, key, location, size \u001b[39m=\u001b[39m data\n\u001b[1;32m    856\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m--> 857\u001b[0m     load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n\u001b[1;32m    858\u001b[0m storage \u001b[39m=\u001b[39m loaded_storages[key]\n\u001b[1;32m    859\u001b[0m \u001b[39mreturn\u001b[39;00m storage\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:846\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    843\u001b[0m dtype \u001b[39m=\u001b[39m data_type(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    845\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, size, dtype)\u001b[39m.\u001b[39mstorage()\n\u001b[0;32m--> 846\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m restore_location(storage, location)\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:151\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 151\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    152\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m             storage_type \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(torch\u001b[39m.\u001b[39mcuda, \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hybridbrep/lib/python3.9/site-packages/torch/serialization.py:135\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    132\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    136\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "repbrep_path = '../../'\n",
    "#model_checkpoint_path = '../../training_logs/reconstruction/old_no_loops/version_0/checkpoints/epoch=38-val_loss=0.002679.ckpt'\n",
    "\n",
    "print('Loading Model')\n",
    "ckpt_path = '/home/ben/Documents/research/repbrep/training_logs/reconstruction/new_with_edges/version_1/checkpoints/epoch=183-val_loss=0.002646.ckpt'\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model = GeneralConvEncDec(64, 1024, 4)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "cube_path = os.path.join(repbrep_path, 'datasets', 'cubes', 'cube.x_t')\n",
    "angled_cube_path = os.path.join(repbrep_path, 'datasets', 'cubes', 'angled_cube.x_t')\n",
    "long_cube_path = os.path.join(repbrep_path, 'datasets', 'cubes', 'long_cube.x_t')\n",
    "\n",
    "frame_guide_1_path = os.path.join(repbrep_path, 'datasets', 'frame_guide', 'fg1.x_t')\n",
    "frame_guide_2_path = os.path.join(repbrep_path, 'datasets', 'frame_guide', 'fg2.x_t')\n",
    "\n",
    "rounded_rect_1_path = os.path.join(repbrep_path, 'datasets', 'rounded_rect', 'rounded_rect1.step')\n",
    "rounded_rect_2_path = os.path.join(repbrep_path, 'datasets', 'rounded_rect', 'rounded_rect2.step')\n",
    "\n",
    "holybox1_path = os.path.join(repbrep_path, 'datasets', 'holybox', 'holey_box.step')\n",
    "holybox2_path = os.path.join(repbrep_path, 'datasets', 'holybox', 'holey_box_angled.step')\n",
    "\n",
    "source_part_path = cube_path\n",
    "target_part_path = angled_cube_path\n",
    "\n",
    "weighted = True\n",
    "num_iters = 300\n",
    "reproject = False\n",
    "\n",
    "sampler = SamplePoints(6*1000)\n",
    "\n",
    "source_part_bak = HPart(source_part_path, normalize=True).data\n",
    "source_part = HPart(source_part_path, normalize=True).data\n",
    "target_part = HPart(target_part_path, normalize=False).data\n",
    "target_part_normalized = HPart(target_part_path, normalize=True).data\n",
    "\n",
    "target_V = source_part.scale*(target_part.V + source_part.translation)\n",
    "target_F = target_part.F\n",
    "target_data = TGData(pos=target_V,face=target_F)\n",
    "sampler(target_data)\n",
    "target_pc = target_data.pos\n",
    "\n",
    "#checkpoint = torch.load(model_checkpoint_path)\n",
    "\n",
    "#model = BRepFaceAutoencoder(64, 1024,4, False)\n",
    "#model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "n_faces = len(source_part.faces)\n",
    "N = 50\n",
    "line = torch.linspace(-0.1,1.1,N)\n",
    "grid = torch.cartesian_prod(line, line)\n",
    "grids = grid.repeat(n_faces,1)\n",
    "grids = grids.reshape((n_faces,-1,2))\n",
    "#indices = torch.arange(n_faces).repeat_interleave(N*N, dim=0)\n",
    "\n",
    "model = model.cuda()\n",
    "grids = grids.cuda()\n",
    "#indices = indices.cuda()\n",
    "target = target_pc.unsqueeze(0).float().cuda()\n",
    "source_part = source_part.cuda()\n",
    "\n",
    "source_part.faces.requires_grad = True\n",
    "source_part.edges.requires_grad = True\n",
    "source_part.vertices.requires_grad = True\n",
    "\n",
    "opt = torch.optim.SGD(\n",
    "    [\n",
    "        source_part.faces,\n",
    "        source_part.edges,\n",
    "        source_part.vertices\n",
    "    ], \n",
    "    lr=0.00075, \n",
    "    momentum=0.3\n",
    ")\n",
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "chamferDist = WeightedChamferDistance(sharpness=2) if weighted else ChamferDistance()\n",
    "\n",
    "radii = []\n",
    "for iter in tqdm(range(num_iters)):\n",
    "    radii.append(source_part.faces[0,17].detach().item())\n",
    "\n",
    "    opt.zero_grad()\n",
    "    _, pred = model(source_part, grids)#, indices)\n",
    "    pred_xyz = pred[:,:3].unsqueeze(0).float()\n",
    "    pred_m = pred[:,3].unsqueeze(0).float()\n",
    "    loss = chamferDist(pred.unsqueeze(0).float(), target) if weighted else chamferDist(pred_xyz, target) + chamferDist(target, pred_xyz)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.detach().item())\n",
    "    predictions.append((pred_xyz.detach().cpu().numpy(), pred_m.detach().cpu().numpy()))\n",
    "radii.append(source_part.faces[0,17].detach().item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.figure()\n",
    "plt.plot(radii)\n",
    "plt.axhline(y=source_part_bak.faces[0,17])\n",
    "plt.axhline(y=target_part_normalized.faces[0,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_cube_path = os.path.join(repbrep_path, 'datasets', 'cubes', 'angled_recon_old_model.x_t')\n",
    "recon_part = HPart(target_part_path, normalize=False).data\n",
    "\n",
    "recon_V = (source_part.scale.to('cpu')*(target_part.V + source_part.translation.to('cpu'))).numpy()\n",
    "recon_F = target_part.F.T.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_V = source_part_bak.V.numpy()\n",
    "source_F = source_part_bak.F.T.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(E.flatten()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i < j:\n",
    "            V_i = source_V[i]\n",
    "            V_j = source_V[j]\n",
    "            if (V_i != V_j).sum() == 1:\n",
    "                E.append([i,j])\n",
    "E = np.array(E)\n",
    "recon_plot = mp.plot(source_V, source_F, return_plot=True)\n",
    "recon_plot.add_points(target_pc.numpy(), shading={'point_color':'red', 'point_size':.2})\n",
    "recon_plot.add_edges(source_V, E, shading={'edge_color':'black','edge_width':0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i < j:\n",
    "            V_i = recon_V[i]\n",
    "            V_j = recon_V[j]\n",
    "            if (V_i != V_j).sum() == 1:\n",
    "                E.append([i,j])\n",
    "E.append([6,7])\n",
    "E.append([4,5])\n",
    "E = np.array(E)\n",
    "recon_plot = mp.plot(recon_V, recon_F, return_plot=True)\n",
    "recon_plot.add_points(target_pc.numpy(), shading={'point_color':'red', 'point_size':.2})\n",
    "recon_plot.add_edges(recon_V, E, shading={'edge_color':'black','edge_width':.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = 10\n",
    "n_preds = len(predictions)\n",
    "plot_steps = torch.linspace(0,n_preds-1,n_plots).int().numpy()\n",
    "\n",
    "for i in plot_steps:\n",
    "    xyz = predictions[i][0][0,:,:]\n",
    "    mask = predictions[i][1][0,:]\n",
    "\n",
    "    plot = mp.plot(xyz, c=(mask <= 0.0), shading={'point_size':0.1}, return_plot=True)\n",
    "    plot.add_points(target_pc.numpy(), shading={'point_size':0.1, 'point_color':'red'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_part.faces[:,:5].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_part.faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = source_part.faces[:,:5].argmax(dim=1).detach().cpu().numpy()\n",
    "origin = source_part.faces[:,5:8].cpu().detach().numpy()\n",
    "normal = source_part.faces[:,8:11].cpu().detach().numpy()\n",
    "axis = source_part.faces[:,11:14].cpu().detach().numpy()\n",
    "ref_dir = source_part.faces[:,14:17].cpu().detach().numpy()\n",
    "radius = source_part.faces[:,17].cpu().detach().numpy()\n",
    "minor_radius = source_part.faces[:,18].cpu().detach().numpy()\n",
    "semi_angle = source_part.faces[:,19].cpu().detach().numpy()\n",
    "flipped = target_part.faces[:,20].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_part_normalized.faces[0,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_part_bak.faces[0,14:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ref_dir[0]**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_num = 1\n",
    "for i in range(len(type)):\n",
    "    p_x, p_y, p_z = origin[i].tolist()\n",
    "    flip = flipped[i]*2 - 1\n",
    "    n_x, n_y, n_z = (flip*normal[i]).tolist()\n",
    "    a_x, a_y, a_z = (flip*axis[i]).tolist()\n",
    "    r = radius[i]\n",
    "    if type[i] == 0:\n",
    "        print(f'{plane_num} Plane( point=({p_x:.4f}, {p_y:.4f}, {p_z:.4f}), normal=({n_x:.4f}, {n_y:.4f}, {n_z:.4f}))\\n')\n",
    "        plane_num += 1\n",
    "    else:\n",
    "        print(f'Cylinder(origin=({p_x:.4f}, {p_y:.4f}, {p_z:.4f}), axis=({a_x:.4f}, {a_y:.4f}, {a_z:.4f}), radius={(r):.4f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_idx = 0\n",
    "for i in range(len(type)):\n",
    "    ori = origin[i].tolist()\n",
    "    norm = normal[i].tolist()\n",
    "    ax = axis[i].tolist()\n",
    "    rad = radius[i]\n",
    "    if type[i] == 0:\n",
    "        print(f'Plane {plane_idx}( origin=({ori[0]:.4f}, {ori[1]:.4f}, {ori[2]:.4f}), normal=({norm[0]:.4f}, {norm[1]:.4f}, {norm[2]:.4f}))\\n')\n",
    "    elif type[i] == 1:\n",
    "        print(f'Cylinder( origin=({ori[0]:.4f}, {ori[1]:.4f}, {ori[2]:.4f}), axis=({ax[0]:.4f}, {ax[1]:.4f}, {ax[2]:.4f}), radius={rad:.4f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(source_part.V.numpy(), source_part.F.T.numpy())\n",
    "\n",
    "mp.plot(target_V.numpy(), target_F.T.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hybridbrep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6aa2ac8926c38c90b9dbe4656179f4fe9d7aa58f584c2a3569efb38019544a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
